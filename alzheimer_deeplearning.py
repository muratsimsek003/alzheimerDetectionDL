# -*- coding: utf-8 -*-
"""Alzheimer_DeepLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MpSewL1949u4QHae2vPWYQyE8Tc-1mX3

#Preprocessing
"""

import warnings
warnings.filterwarnings('ignore')

import os
from os import listdir
import pathlib
from random import randint
import numpy as np
from numpy import asarray
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.image import imread
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import tensorflow as tf
import keras
import os
from distutils.dir_util import copy_tree, remove_tree

from keras.utils import load_img,img_to_array
from keras.models import Sequential
from keras.layers import MaxPooling2D,Dropout,Dense,Input,Conv2D,Flatten,Conv2DTranspose
from keras.layers import GlobalAveragePooling2D,MaxPool2D,BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.utils import plot_model

print("TensorFlow Version:", tf.__version__)

base_dir = "dataset/"

test_dir = base_dir + "test/"
train_dir = base_dir + "train/"
work_dir = "datasets/"

if os.path.exists(work_dir):
    remove_tree(work_dir)


os.mkdir(work_dir)
copy_tree(train_dir, work_dir)
copy_tree(test_dir, work_dir)
print("Working Directory Contents:", os.listdir(work_dir))


WORK_DIR = './datasets/'

CLASSES = [ 'NonDemented',
            'VeryMildDemented',
            'MildDemented',
            'ModerateDemented']

IMG_SIZE = 128
IMAGE_SIZE = [128, 128]
DIM = (IMG_SIZE, IMG_SIZE)

ZOOM = [.99, 1.01]
BRIGHT_RANGE = [0.8, 1.2]
HORZ_FLIP = True
FILL_MODE = "constant"
DATA_FORMAT = "channels_last"

work_dr = IDG(rescale = 1./255, brightness_range=BRIGHT_RANGE, zoom_range=ZOOM, data_format=DATA_FORMAT, fill_mode=FILL_MODE, horizontal_flip=HORZ_FLIP)

data_gen = work_dr.flow_from_directory(directory=WORK_DIR, target_size=DIM, batch_size=6500, shuffle=False)

def show_images(generator,y_pred=None):
    """
    Input: An image generator,predicted labels (optional)
    Output: Displays a grid of 9 images with lables
    """

    # get image lables
    labels =dict(zip([0,1,2,3], CLASSES))

    # get a batch of images
    x,y = generator.next()

    # display a grid of 9 images
    plt.figure(figsize=(10, 10))
    if y_pred is None:
        for i in range(9):
            ax = plt.subplot(3, 3, i + 1)
            idx = randint(0, 6400)
            plt.imshow(x[idx])
            plt.axis("off")
            plt.title("Class:{}".format(labels[np.argmax(y[idx])]))


    else:
        for i in range(9):
            ax = plt.subplot(3, 3, i + 1)
            plt.imshow(x[i])
            plt.axis("off")
            plt.title("Actual:{} \nPredicted:{}".format(labels[np.argmax(y[i])],labels[y_pred[i]]))
    plt.show()

# Display Train Images
show_images(data_gen)

#Retrieving the data from the ImageDataGenerator iterator

data, labels = data_gen.next()

#Getting to know the dimensions of our dataset

print(data.shape, labels.shape)

#Splitting the data into train, test, and validation sets

train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = 0.2, random_state=42)
train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size = 0.2, random_state=42)



# Defining convolutional blocks
def conv_block(filters, act='relu'):

    block = Sequential()
    block.add(Conv2D(filters, 3, activation=act, padding='same'))
    block.add(Conv2D(filters, 3, activation=act, padding='same'))
    block.add(BatchNormalization())
    block.add(MaxPool2D())

    return block

# defining dense blocks
def dense_block(units, dropout_rate, act='relu'):
    block = Sequential()
    block.add(Dense(units, activation=act))
    block.add(BatchNormalization())
    block.add(Dropout(dropout_rate))

    return block

IMAGE_SIZE = [128,128]
act = 'relu'

model1 = Sequential([
        Input(shape=(*IMAGE_SIZE, 3)),
        Conv2D(16, 3, activation=act, padding='same'),
        Conv2D(16, 3, activation=act, padding='same'),
        MaxPool2D(),
        conv_block(32),
        conv_block(64),
        conv_block(128),
        Dropout(0.2),
        conv_block(256),
        Dropout(0.2),
        Flatten(),
        dense_block(512, 0.7),
        dense_block(128, 0.5),
        dense_block(64, 0.3),
        Dense(4, activation='softmax')
    ], name = "cnn_model")

METRICS = [tf.keras.metrics.CategoricalAccuracy(name='acc'),
           tf.keras.metrics.AUC(name='auc'),
           tf.keras.metrics.Precision(name="prec"),
           tf.keras.metrics.Recall(name="recall")]

model1.compile(optimizer='adam',
              loss=tf.losses.CategoricalCrossentropy(),
              metrics=METRICS)

model1.summary()

plot_model(model1)

CALLBACKS = [EarlyStopping(monitor='acc', min_delta=0.01, patience=5, mode='max')]

EPOCHS = 50

history = model1.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=EPOCHS,callbacks=CALLBACKS)

train_accuracy = history.history['acc']
val_accuracy = history.history['val_acc']

train_loss = history.history['loss']
val_loss = history.history['val_loss']

train_auc = history.history['auc']
val_auc = history.history['val_auc']

fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 10))

ax[0].set_title('Training Accuracy vs. Epochs')
ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')
ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')
ax[0].legend(loc='best')

ax[1].set_title('Training/Validation Loss vs. Epochs')
ax[1].plot(train_loss, 'o-', label='Train Loss')
ax[1].plot(val_loss, 'o-', label='Validation Loss')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')
ax[1].legend(loc='best')

ax[2].set_title('Training AUC vs. Epochs')
ax[2].plot(train_auc, 'o-', label='Train AUC')
ax[2].plot(val_auc, 'o-', label='Validation AUC')
ax[2].set_xlabel('Epochs')
ax[2].set_ylabel('AUC')
ax[2].legend(loc='best')

plt.tight_layout()
plt.show()

test_scores = model1.evaluate(test_data, test_labels)
print("Testing Accuracy: %.2f%%"%(test_scores[1] * 100))

"""#SMOTE Handling Imbalance Dataset Method"""

sm = SMOTE(random_state=42)

smote_data, smote_labels = sm.fit_resample(data.reshape(-1, IMG_SIZE * IMG_SIZE * 3), labels)

print(smote_data.shape, smote_labels.shape)

smote_data = smote_data.reshape(-1, IMG_SIZE, IMG_SIZE, 3)
print(smote_data.shape, smote_labels.shape)

#Splitting the data into train, test, and validation sets

smote_train_data, smote_test_data, smote_train_labels, smote_test_labels = train_test_split(smote_data, smote_labels, test_size = 0.2, random_state=42)
smote_train_data, smote_val_data, smote_train_labels, smote_val_labels = train_test_split(smote_data, smote_labels, test_size = 0.2, random_state=42)

EPOCHS = 50

smote_history = model1.fit(smote_train_data, smote_train_labels, validation_data=(smote_val_data, smote_val_labels), epochs=EPOCHS,callbacks=CALLBACKS)